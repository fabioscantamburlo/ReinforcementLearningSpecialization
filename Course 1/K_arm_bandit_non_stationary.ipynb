{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KArmedBandit_NS:\n",
    "    def __init__(self, n_of_arms, mean, variance, epsilon, alpha, seed):\n",
    "        self._n_of_arms = n_of_arms\n",
    "        self._mean = mean\n",
    "        self._variance = variance\n",
    "        self._epsilon = epsilon\n",
    "        self._alpha = alpha\n",
    "        self._seed = seed\n",
    "        np.random.seed(self._seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._q_fs = [0 for _ in range(self._n_of_arms)]\n",
    "        self._q_icr = [0 for _ in range(self._n_of_arms)]\n",
    "        self._reward_num = [0 for _ in range(self._n_of_arms)]\n",
    "        self._reward_hist = []\n",
    "\n",
    "    def select_action(self):\n",
    "        if np.random.uniform() > self._epsilon:\n",
    "            # print(\"Greedy logic\")\n",
    "            action = np.argmax(self._q_icr)\n",
    "        else:\n",
    "            # print(\"Random epsilon\")\n",
    "            action = np.random.choice(self._n_of_arms)\n",
    "            # print(f\"Selected action: {action}\")\n",
    "        return action\n",
    "\n",
    "    def generate_reward(self, action, iter):\n",
    "        \n",
    "        # After 100 iterations the env become non stationary\n",
    "        if iter > 100:\n",
    "            value = self.update_qs_generate_non_stationary_value(action)\n",
    "        value = np.random.normal(self._mean[action], self._variance[action])\n",
    "        print(f\"Got value = {value}\")\n",
    "        return value\n",
    "    \n",
    "    def update_qs_generate_non_stationary_value(self, action):\n",
    "        self._mean[action] += np.random.normal(0, 0.01)\n",
    "\n",
    "    def update_internal_state(self, action, reward):\n",
    "        # update Q, reward num\n",
    "        # print(\"Updating value\")\n",
    "        print(f\"Previous value fs = {self._q_fs[action]}\")\n",
    "        print(f\"Previous value iqr = {self._q_icr[action]}\")\n",
    "        print(f\"Previous reward num = {self._reward_num[action]}\")\n",
    "\n",
    "        self._reward_num[action] += 1\n",
    "        \n",
    "        self._q_fs[action] = ((self._q_fs[action] * (self._reward_num[action] - self._alpha)) + (self._alpha * reward)) / self._reward_num[action]\n",
    "        \n",
    "        self._q_icr[action] = self._q_icr[action] + ((self._alpha*(reward - self._q_icr[action])) / (self._reward_num[action]))\n",
    "        \n",
    "        print(f\"new_q_fs = {self._q_fs[action]}\")\n",
    "        print(f\"new_q_iqr = {self._q_icr[action]}\")\n",
    "\n",
    "    def run(self, n_of_iter):\n",
    "        sum_rewards = 0\n",
    "        for _i in range(n_of_iter):\n",
    "        \n",
    "            action = self.select_action()    \n",
    "            reward = self.generate_reward(action=action, iter=_i)\n",
    "            sum_rewards += reward\n",
    "            print(f\"sum rewards = {sum_rewards}\")\n",
    "            self._reward_hist.append(sum_rewards/(_i+1))\n",
    "\n",
    "            self.update_internal_state(action=action, reward=reward)\n",
    "    \n",
    "    def summarize(self):\n",
    "        print(\"Summarize bandit\")\n",
    "        print(\"Q function and reward num:\")\n",
    "        print(self._q_fs)\n",
    "        print(self._q_icr)\n",
    "        print(self._reward_num)\n",
    "        print(\"Mean changes\")\n",
    "        print(self._mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "variance = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "n_experiments = 100_000\n",
    "\n",
    "n_exp = {'a': 0.1, 'b': 0.1, 'c': 0.1, 'd': 0.1, 'e': 0.1}\n",
    "bandits = {name: KArmedBandit_NS(n_of_arms=10, mean=mean, variance=variance, epsilon=0.1, alpha=value, seed=42) for name, value in n_exp.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "for name, bandit in bandits.items():\n",
    "    bandit.reset()\n",
    "    bandit.run(n_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _k in bandits:\n",
    "    print(f\"Bandit {_k} summary:\")\n",
    "    bandits[_k].summarize()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandits['d'].summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "col_list = ['ro', 'bo', 'yo', 'go', 'co']\n",
    "for (name, bandit), col in zip(bandits.items(), col_list):\n",
    "    ax.plot(bandit._reward_hist, col, markersize=2,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
