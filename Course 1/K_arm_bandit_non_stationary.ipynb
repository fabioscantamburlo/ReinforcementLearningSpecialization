{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self, mean, variance, deturpation, seed):\n",
    "        self._seed = seed\n",
    "        self._mean_bkp = mean\n",
    "        self._deturpation = deturpation\n",
    "        self._variance_bkp = variance\n",
    "        self._mean = copy.deepcopy(self._mean_bkp)\n",
    "        self._variance = copy.deepcopy(self._variance_bkp)\n",
    "        self._rng = np.random.default_rng(self._seed)\n",
    "        self._hist_best_reward = []\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        self._mean = copy.deepcopy(self._mean_bkp)\n",
    "        self._variance = copy.deepcopy(self._mean)\n",
    "        self._rng = np.random.default_rng(self._seed)\n",
    "        self._hist_best_reward = []\n",
    "\n",
    "    def add_noise(self):\n",
    "        self._mean = [i + self._rng.normal(0, self._deturpation) for i in self._mean]\n",
    "\n",
    "    def generate_reward(self, action):\n",
    "        self._hist_best_reward.append(np.max(self._mean))\n",
    "        return self._rng.normal(self._mean[action], self._variance[action])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KArmedBandit_NS:\n",
    "    def __init__(self, n_of_arms, stationarity_time, env, epsilon, alpha, k, seed):\n",
    "        self._n_of_arms = n_of_arms\n",
    "        self._stationarity_time = stationarity_time\n",
    "        self._env = env\n",
    "        self._epsilon = epsilon\n",
    "        self._alpha = alpha\n",
    "        self._seed = seed\n",
    "        self._k = k\n",
    "        self._rng = np.random.default_rng(self._seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._q_icr = [0 for _ in range(self._n_of_arms)]\n",
    "        self._reward_num = [0 for _ in range(self._n_of_arms)]\n",
    "        self._mean_reward_hist = []\n",
    "        self._mean_reward_hist_last_k = []\n",
    "        self._reward_hist = []\n",
    "        self._last_k_reward_hist = []\n",
    "\n",
    "    def select_action(self):\n",
    "        if self._rng.uniform() > self._epsilon:\n",
    "            # print(\"Greedy logic\")\n",
    "            action = np.argmax(self._q_icr)\n",
    "        else:\n",
    "            # print(\"Random epsilon\")\n",
    "            action = self._rng.choice(self._n_of_arms)\n",
    "            # print(f\"Selected action: {action}\")\n",
    "        return action\n",
    "\n",
    "    def generate_reward(self, action):\n",
    "        \n",
    "        value = self._env.generate_reward(action)\n",
    "        print(self._env._mean)\n",
    "        print(f\"Got value = {value}\")\n",
    "        return value\n",
    "\n",
    "\n",
    "    def update_internal_state(self, action, reward):\n",
    "        # update Q, reward num\n",
    "        # print(\"Updating value\")\n",
    "        #print(f\"Previous value fs = {self._q_fs[action]}\")\n",
    "        print(f\"Previous value iqr = {self._q_icr[action]}\")\n",
    "        print(f\"Previous reward num = {self._reward_num[action]}\")\n",
    "\n",
    "        self._reward_num[action] += 1\n",
    "        \n",
    "        self._q_icr[action] = self._q_icr[action] + ((self._alpha*(reward - self._q_icr[action])) / (self._reward_num[action]))\n",
    "\n",
    "        #self._q_icr[action] = self._q_icr[action] + (reward - self._q_icr[action]) / (self._reward_num[action])\n",
    "        \n",
    "        print(f\"new_q_iqr = {self._q_icr[action]}\")\n",
    "\n",
    "    def run(self, n_of_iter):\n",
    "        sum_rewards = 0\n",
    "        for _i in range(n_of_iter):\n",
    "\n",
    "            if _i > self._stationarity_time:\n",
    "                self._env.add_noise()\n",
    "        \n",
    "            action = self.select_action()    \n",
    "            reward = self.generate_reward(action=action)\n",
    "            sum_rewards += reward\n",
    "            print(f\"sum rewards = {sum_rewards}\")\n",
    "            #self._reward_hist.append(reward)\n",
    "            self._mean_reward_hist.append(sum_rewards/(_i+1))\n",
    "            \n",
    "            self._last_k_reward_hist.append(reward)\n",
    "            if self._k == len(self._last_k_reward_hist):\n",
    "                self._last_k_reward_hist.pop(0)\n",
    "            \n",
    "            self._mean_reward_hist_last_k.append(np.average(self._last_k_reward_hist))\n",
    "            self.update_internal_state(action=action, reward=reward)\n",
    "    \n",
    "    def summarize(self):\n",
    "        print(\"Summarize bandit\")\n",
    "        print(\"Q function and reward num:\")\n",
    "        print(self._q_icr)\n",
    "        print(self._reward_num)\n",
    "        print(\"Mean changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [1, 1, 1, 2, 1, 1, 1, 1, 1, 1]\n",
    "variance = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "n_experiments = 30_000\n",
    "\n",
    "# n_exp = {'a': 0.8, 'b': 0.7, 'c': 0.6, 'd': 0.5, 'e': 0.2}\n",
    "\n",
    "n_exp = {'a': 0.4, 'b': 0.3, 'c': 0.2, 'd': 0.15, 'e': 0.1}\n",
    "\n",
    "envs = {name: Environment(mean, variance, deturpation=0.1, seed=42) for name, _ in n_exp.items()}\n",
    "bandits = {name: KArmedBandit_NS(n_of_arms=10, stationarity_time=200, env=envs[name], epsilon=value, alpha=0.01, k=200, seed=42) for name, value in n_exp.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "d = {}\n",
    "for name, bandit in bandits.items():\n",
    "    bandit.run(n_experiments)\n",
    "    d[name] = envs[name]._mean\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bandits['e']._reward_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _k in bandits:\n",
    "#     print(f\"Bandit {_k} summary:\")\n",
    "#     bandits[_k].summarize()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "col_list = ['ro', 'bo', 'yo', 'go', 'co']\n",
    "for (name, bandit), col in zip(bandits.items(), col_list):\n",
    "    ax.plot(bandit._mean_reward_hist, col, markersize=2,)\n",
    "ax.plot(envs['a']._hist_best_reward, 'g+', markersize=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "col_list = ['ro', 'bo', 'yo', 'go', 'co']\n",
    "for (name, bandit), col in zip(bandits.items(), col_list):\n",
    "    ax.plot(bandit._mean_reward_hist_last_k, col, markersize=2,)\n",
    "\n",
    "ax.plot(envs['a']._hist_best_reward, 'g+', markersize=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
